{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfQgEtOA2ubf",
        "outputId": "beb6c15b-178d-4fd6-e2ec-3041bd6799c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snowflake-connector-python in /usr/local/lib/python3.10/dist-packages (3.12.2)\n",
            "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.5.1)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (43.0.1)\n",
            "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.9.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2024.2)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.12.2)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.16.1)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.3.6)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (0.13.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->snowflake-connector-python) (2.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install snowflake-connector-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-Li6HcL1JJN"
      },
      "outputs": [],
      "source": [
        "import snowflake.connector\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KyFUqmu1pyB"
      },
      "outputs": [],
      "source": [
        "# Snowflake connection parameters\n",
        "SNOWFLAKE_ACCOUNT = 'bw02678.ap-southeast-1'\n",
        "SNOWFLAKE_USER = 'SANTHOSHKRISH03'\n",
        "SNOWFLAKE_PASSWORD = 'Santhosh_2003.'\n",
        "SNOWFLAKE_WAREHOUSE = 'MUSIC_ANALYTICS_WH'\n",
        "SNOWFLAKE_DATABASE = 'MUSIC_ANALYTICS'\n",
        "SNOWFLAKE_SCHEMA = 'RAW_DATA'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPpPZiwj1r1F"
      },
      "outputs": [],
      "source": [
        "def connect_to_snowflake():\n",
        "    return snowflake.connector.connect(\n",
        "        account=SNOWFLAKE_ACCOUNT,\n",
        "        user=SNOWFLAKE_USER,\n",
        "        password=SNOWFLAKE_PASSWORD,\n",
        "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
        "        database=SNOWFLAKE_DATABASE,\n",
        "        schema=SNOWFLAKE_SCHEMA\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_query(conn, query, params=None):\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(query, params)\n",
        "    conn.commit()\n",
        "    cursor.close()"
      ],
      "metadata": {
        "id": "3Cew8QI_pkXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mdm_tables(conn):\n",
        "    queries = [\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS master_artists (\n",
        "            artist_id VARCHAR(36) DEFAULT UUID_STRING(),\n",
        "            artist_name VARCHAR(255) NOT NULL,\n",
        "            spotify_id VARCHAR(255),\n",
        "            twitter_handle VARCHAR(255),\n",
        "            created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
        "            updated_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
        "            PRIMARY KEY (artist_id),\n",
        "            UNIQUE (artist_name)\n",
        "        )\n",
        "        \"\"\",\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS master_songs (\n",
        "            song_id VARCHAR(36) DEFAULT UUID_STRING(),\n",
        "            song_name VARCHAR(255) NOT NULL,\n",
        "            artist_id VARCHAR(36),\n",
        "            album_name VARCHAR(255),\n",
        "            spotify_id VARCHAR(255),\n",
        "            created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
        "            updated_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
        "            PRIMARY KEY (song_id),\n",
        "            UNIQUE (spotify_id),\n",
        "            FOREIGN KEY (artist_id) REFERENCES master_artists(artist_id)\n",
        "        )\n",
        "        \"\"\",\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS master_venues (\n",
        "            venue_id VARCHAR(36) DEFAULT UUID_STRING(),\n",
        "            venue_name VARCHAR(255) NOT NULL,\n",
        "            location VARCHAR(255),\n",
        "            created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
        "            updated_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
        "            PRIMARY KEY (venue_id),\n",
        "            UNIQUE (venue_name)\n",
        "        )\n",
        "        \"\"\"\n",
        "    ]\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "        futures = [executor.submit(execute_query, conn, query) for query in queries]\n",
        "        for future in as_completed(futures):\n",
        "            future.result()"
      ],
      "metadata": {
        "id": "GQBd5RcApsnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_upsert_artists(conn, artists_data):\n",
        "    query = \"\"\"\n",
        "    MERGE INTO master_artists AS target\n",
        "    USING (\n",
        "        SELECT column1 as artist_name, column2 as spotify_id, column3 as twitter_handle\n",
        "        FROM VALUES (%s, %s, %s)\n",
        "    ) AS source\n",
        "    ON target.artist_name = source.artist_name\n",
        "    WHEN MATCHED THEN\n",
        "        UPDATE SET\n",
        "            spotify_id = COALESCE(target.spotify_id, source.spotify_id),\n",
        "            twitter_handle = COALESCE(target.twitter_handle, source.twitter_handle),\n",
        "            updated_at = CURRENT_TIMESTAMP()\n",
        "    WHEN NOT MATCHED THEN\n",
        "        INSERT (artist_name, spotify_id, twitter_handle)\n",
        "        VALUES (source.artist_name, source.spotify_id, source.twitter_handle)\n",
        "    \"\"\"\n",
        "\n",
        "    cursor = conn.cursor()\n",
        "    cursor.executemany(query, artists_data)\n",
        "    conn.commit()\n",
        "    cursor.close()\n"
      ],
      "metadata": {
        "id": "nvJsVJGopvcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_mdm_from_spotify(conn):\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Update artists (keep this part unchanged)\n",
        "    cursor.execute(\"\"\"\n",
        "    MERGE INTO master_artists AS target\n",
        "    USING (\n",
        "        SELECT DISTINCT\n",
        "            artists AS artist_name,\n",
        "            FIRST_VALUE(track_id) OVER (PARTITION BY artists ORDER BY track_id) AS spotify_id\n",
        "        FROM RAW_DATA.SPOTIFY_TRACKS\n",
        "        WHERE artists IS NOT NULL\n",
        "    ) AS source\n",
        "    ON target.artist_name = source.artist_name\n",
        "    WHEN MATCHED THEN\n",
        "        UPDATE SET\n",
        "            spotify_id = CASE\n",
        "                WHEN target.spotify_id IS NULL OR target.spotify_id = 'UNKNOWN'\n",
        "                THEN source.spotify_id\n",
        "                ELSE target.spotify_id\n",
        "            END,\n",
        "            updated_at = CURRENT_TIMESTAMP()\n",
        "    WHEN NOT MATCHED THEN\n",
        "        INSERT (artist_name, spotify_id)\n",
        "        VALUES (source.artist_name, source.spotify_id)\n",
        "    \"\"\")\n",
        "\n",
        "    # Update songs (keep this part unchanged)\n",
        "    cursor.execute(\"\"\"\n",
        "    MERGE INTO master_songs AS target\n",
        "    USING (\n",
        "        SELECT DISTINCT\n",
        "            COALESCE(s.track_name, 'UNKNOWN') AS song_name,\n",
        "            a.artist_id,\n",
        "            COALESCE(s.album_name, 'UNKNOWN') AS album_name,\n",
        "            s.track_id AS spotify_id\n",
        "        FROM RAW_DATA.SPOTIFY_TRACKS s\n",
        "        JOIN master_artists a ON a.artist_name = s.artists\n",
        "        WHERE s.track_name IS NOT NULL AND s.artists IS NOT NULL\n",
        "        QUALIFY ROW_NUMBER() OVER (PARTITION BY s.track_id ORDER BY s.track_name) = 1\n",
        "    ) AS source\n",
        "    ON target.spotify_id = source.spotify_id\n",
        "    WHEN MATCHED THEN\n",
        "        UPDATE SET\n",
        "            song_name = CASE WHEN target.song_name = 'UNKNOWN' THEN source.song_name ELSE target.song_name END,\n",
        "            artist_id = source.artist_id,\n",
        "            album_name = CASE WHEN target.album_name = 'UNKNOWN' THEN source.album_name ELSE target.album_name END,\n",
        "            updated_at = CURRENT_TIMESTAMP()\n",
        "    WHEN NOT MATCHED THEN\n",
        "        INSERT (song_name, artist_id, album_name, spotify_id)\n",
        "        VALUES (source.song_name, source.artist_id, source.album_name, source.spotify_id)\n",
        "    \"\"\")\n",
        "\n",
        "    # Update venues (corrected to use the LOCATION field)\n",
        "    cursor.execute(\"\"\"\n",
        "    MERGE INTO master_venues AS target\n",
        "    USING (\n",
        "        SELECT DISTINCT\n",
        "            Venue AS venue_name,\n",
        "            Location AS location\n",
        "        FROM RAW_DATA.CONCERTS\n",
        "        WHERE Venue IS NOT NULL\n",
        "    ) AS source\n",
        "    ON target.venue_name = source.venue_name\n",
        "    WHEN MATCHED THEN\n",
        "        UPDATE SET\n",
        "            location = CASE\n",
        "                WHEN target.location IS NULL OR target.location = 'UNKNOWN'\n",
        "                THEN source.location\n",
        "                ELSE target.location\n",
        "            END,\n",
        "            updated_at = CURRENT_TIMESTAMP()\n",
        "    WHEN NOT MATCHED THEN\n",
        "        INSERT (venue_name, location)\n",
        "        VALUES (source.venue_name, source.location)\n",
        "    \"\"\")\n",
        "\n",
        "    conn.commit()\n",
        "    cursor.close()"
      ],
      "metadata": {
        "id": "AbhT45dIpyTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_quality_table(conn):\n",
        "    query = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS data_quality_issues (\n",
        "        issue_id VARCHAR(36) DEFAULT UUID_STRING(),\n",
        "        source_table VARCHAR(255),\n",
        "        column_name VARCHAR(255),\n",
        "        issue_type VARCHAR(255),\n",
        "        issue_description VARCHAR(1000),\n",
        "        record_count INT,\n",
        "        created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
        "        PRIMARY KEY (issue_id)\n",
        "    )\n",
        "    \"\"\"\n",
        "    execute_query(conn, query)\n"
      ],
      "metadata": {
        "id": "74JLpwA0p0Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def profile_data(conn, table_name: str):\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(f\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = UPPER('{table_name}')\")\n",
        "    columns = [row[0] for row in cursor.fetchall()]\n",
        "    cursor.close()\n",
        "\n",
        "    queries = []\n",
        "    for column_name in columns:\n",
        "        queries.append(f\"\"\"\n",
        "        INSERT INTO data_quality_issues (source_table, column_name, issue_type, issue_description, record_count)\n",
        "        SELECT '{table_name}', '{column_name}', 'NULL_VALUES', 'Column contains null values', COUNT(*)\n",
        "        FROM {table_name}\n",
        "        WHERE {column_name} IS NULL\n",
        "        HAVING COUNT(*) > 0\n",
        "        \"\"\")\n",
        "\n",
        "        queries.append(f\"\"\"\n",
        "        INSERT INTO data_quality_issues (source_table, column_name, issue_type, issue_description, record_count)\n",
        "        SELECT '{table_name}', '{column_name}', 'DUPLICATE_VALUES', 'Column contains duplicate values', COUNT(*) - COUNT(DISTINCT {column_name})\n",
        "        FROM {table_name}\n",
        "        HAVING COUNT(*) - COUNT(DISTINCT {column_name}) > 0\n",
        "        \"\"\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        futures = [executor.submit(execute_query, conn, query) for query in queries]\n",
        "        for future in as_completed(futures):\n",
        "            future.result()"
      ],
      "metadata": {
        "id": "y1ORkNA-p65x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def profile_data_batch(conn, tables):\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    for table in tables:\n",
        "        # Get the column names for the table\n",
        "        cursor.execute(f\"\"\"\n",
        "            SELECT COLUMN_NAME\n",
        "            FROM INFORMATION_SCHEMA.COLUMNS\n",
        "            WHERE TABLE_NAME = '{table.split('.')[-1]}'\n",
        "            AND TABLE_SCHEMA = '{table.split('.')[0]}'\n",
        "        \"\"\")\n",
        "        columns = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "        # Create individual queries for each column\n",
        "        for column in columns:\n",
        "            # Check for NULL values\n",
        "            null_query = f\"\"\"\n",
        "            INSERT INTO data_quality_issues (source_table, column_name, issue_type, issue_description, record_count)\n",
        "            SELECT\n",
        "                '{table}' as source_table,\n",
        "                '{column}' as column_name,\n",
        "                'NULL_VALUES' as issue_type,\n",
        "                'Column contains null values' as issue_description,\n",
        "                COUNT(*) as record_count\n",
        "            FROM {table}\n",
        "            WHERE {column} IS NULL\n",
        "            HAVING COUNT(*) > 0\n",
        "            \"\"\"\n",
        "            cursor.execute(null_query)\n",
        "\n",
        "            # Check for duplicate values\n",
        "            duplicate_query = f\"\"\"\n",
        "            INSERT INTO data_quality_issues (source_table, column_name, issue_type, issue_description, record_count)\n",
        "            SELECT\n",
        "                '{table}' as source_table,\n",
        "                '{column}' as column_name,\n",
        "                'DUPLICATE_VALUES' as issue_type,\n",
        "                'Column contains duplicate values' as issue_description,\n",
        "                COUNT(*) - COUNT(DISTINCT {column}) as record_count\n",
        "            FROM {table}\n",
        "            HAVING COUNT(*) - COUNT(DISTINCT {column}) > 0\n",
        "            \"\"\"\n",
        "            cursor.execute(duplicate_query)\n",
        "\n",
        "    conn.commit()\n",
        "    cursor.close()"
      ],
      "metadata": {
        "id": "eALzR622ssJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_lineage_table(conn):\n",
        "    query = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS data_lineage (\n",
        "        lineage_id VARCHAR(36) DEFAULT UUID_STRING(),\n",
        "        source_table VARCHAR(255),\n",
        "        source_column VARCHAR(255),\n",
        "        target_table VARCHAR(255),\n",
        "        target_column VARCHAR(255),\n",
        "        transformation_description VARCHAR(1000),\n",
        "        created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
        "        updated_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
        "        PRIMARY KEY (lineage_id)\n",
        "    )\n",
        "    \"\"\"\n",
        "    execute_query(conn, query)"
      ],
      "metadata": {
        "id": "K8v94Ob8p9il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_update_data_lineage(conn, lineage_data):\n",
        "    query = \"\"\"\n",
        "    MERGE INTO data_lineage AS target\n",
        "    USING (\n",
        "        SELECT\n",
        "            column1 as source_table,\n",
        "            column2 as source_column,\n",
        "            column3 as target_table,\n",
        "            column4 as target_column,\n",
        "            column5 as transformation_description\n",
        "        FROM VALUES (%s, %s, %s, %s, %s)\n",
        "    ) AS source\n",
        "    ON target.source_table = source.source_table\n",
        "    AND target.source_column = source.source_column\n",
        "    AND target.target_table = source.target_table\n",
        "    AND target.target_column = source.target_column\n",
        "    WHEN MATCHED THEN\n",
        "        UPDATE SET\n",
        "            transformation_description = source.transformation_description,\n",
        "            updated_at = CURRENT_TIMESTAMP()\n",
        "    WHEN NOT MATCHED THEN\n",
        "        INSERT (source_table, source_column, target_table, target_column, transformation_description)\n",
        "        VALUES (source.source_table, source.source_column, source.target_table, source.target_column, source.transformation_description)\n",
        "    \"\"\"\n",
        "\n",
        "    cursor = conn.cursor()\n",
        "    cursor.executemany(query, lineage_data)\n",
        "    conn.commit()\n",
        "    cursor.close()"
      ],
      "metadata": {
        "id": "NBF_wZdWqDd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_impact_analysis(conn, source_table: str, source_column: str):\n",
        "    query = \"\"\"\n",
        "    WITH RECURSIVE impact_tree AS (\n",
        "        SELECT target_table, target_column, transformation_description\n",
        "        FROM data_lineage\n",
        "        WHERE source_table = %s AND source_column = %s\n",
        "        UNION ALL\n",
        "        SELECT dl.target_table, dl.target_column, dl.transformation_description\n",
        "        FROM data_lineage dl\n",
        "        JOIN impact_tree it ON dl.source_table = it.target_table AND dl.source_column = it.target_column\n",
        "    )\n",
        "    SELECT DISTINCT * FROM impact_tree\n",
        "    \"\"\"\n",
        "\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(query, (source_table, source_column))\n",
        "    results = cursor.fetchall()\n",
        "    cursor.close()\n",
        "    return results"
      ],
      "metadata": {
        "id": "j6uTJKgNqGKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    start_time = time.time()\n",
        "    conn = connect_to_snowflake()\n",
        "\n",
        "    try:\n",
        "        print(\"Creating MDM tables...\")\n",
        "        create_mdm_tables(conn)\n",
        "\n",
        "        print(\"Updating MDM from Spotify...\")\n",
        "        update_mdm_from_spotify(conn)\n",
        "\n",
        "        print(\"Creating data quality table...\")\n",
        "        create_data_quality_table(conn)\n",
        "\n",
        "        print(\"Profiling data...\")\n",
        "        tables_to_profile = ['RAW_DATA.SPOTIFY_TRACKS', 'RAW_DATA.BILLBOARD_HOT_100', 'RAW_DATA.TWEETS', 'RAW_DATA.CONCERTS']\n",
        "        profile_data_batch(conn, tables_to_profile)\n",
        "\n",
        "        print(\"Creating data lineage table...\")\n",
        "        create_data_lineage_table(conn)\n",
        "\n",
        "        print(\"Updating data lineage...\")\n",
        "        lineage_data = [\n",
        "            ('RAW_DATA.SPOTIFY_TRACKS', 'artists', 'master_artists', 'artist_name', 'Direct mapping from Spotify tracks to master artists'),\n",
        "            ('RAW_DATA.SPOTIFY_TRACKS', 'track_name', 'master_songs', 'song_name', 'Direct mapping from Spotify tracks to master songs'),\n",
        "            ('RAW_DATA.BILLBOARD_HOT_100', 'artist', 'master_artists', 'artist_name', 'Direct mapping from Billboard to master artists'),\n",
        "            ('RAW_DATA.BILLBOARD_HOT_100', 'song', 'master_songs', 'song_name', 'Direct mapping from Billboard to master songs'),\n",
        "            ('RAW_DATA.TWEETS', 'Username', 'master_artists', 'twitter_handle', 'Potential mapping from Twitter username to artist'),\n",
        "            ('RAW_DATA.CONCERTS', 'Main_Artist', 'master_artists', 'artist_name', 'Direct mapping from Concerts to master artists'),\n",
        "            ('RAW_DATA.CONCERTS', 'Venue', 'master_venues', 'venue_name', 'Direct mapping from Concerts to master venues')\n",
        "        ]\n",
        "        batch_update_data_lineage(conn, lineage_data)\n",
        "\n",
        "        print(\"Performing impact analysis...\")\n",
        "        impact_results = perform_impact_analysis(conn, 'RAW_DATA.SPOTIFY_TRACKS', 'artists')\n",
        "        print(\"Impact analysis results for Spotify artists:\")\n",
        "        for result in impact_results:\n",
        "            print(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        conn.rollback()\n",
        "    else:\n",
        "        conn.commit()\n",
        "    finally:\n",
        "        conn.close()\n",
        "        end_time = time.time()\n",
        "        print(f\"MDM implementation completed in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-xh9rOOqIaH",
        "outputId": "5cf47454-7e27-4e44-8b17-0f7cfb4f256a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating MDM tables...\n",
            "Updating MDM from Spotify...\n",
            "Creating data quality table...\n",
            "Profiling data...\n",
            "Creating data lineage table...\n",
            "Updating data lineage...\n",
            "Performing impact analysis...\n",
            "Impact analysis results for Spotify artists:\n",
            "('master_artists', 'artist_name', 'Direct mapping from Spotify tracks to master artists')\n",
            "MDM implementation completed in 65.35 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}